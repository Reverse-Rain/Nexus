{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0TPhhMPJGJAlmiljMB4wC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Reverse-Rain/Nexus/blob/main/base_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import math\n",
        "import spacy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models import FastText\n",
        "from laserembeddings import Laser\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class SimilarityModels:\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.model_paraphrase = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "        self.module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "        self.use_model = hub.load(self.module_url)\n",
        "        self.model_sbert = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
        "        self.model_distilbert = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "        self.laser_model = Laser()\n",
        "\n",
        "    def preprocess_text(self, sentence):\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        word_tokens = word_tokenize(sentence.lower())\n",
        "        filtered_sentence = [word for word in word_tokens if word not in stop_words]\n",
        "        return filtered_sentence\n",
        "\n",
        "    def get_cosine_similarity(self, sentence1, sentence2):\n",
        "        sentence1 = self.preprocess_text(sentence1)\n",
        "        sentence2 = self.preprocess_text(sentence2)\n",
        "\n",
        "        vector1 = Counter(sentence1)\n",
        "        vector2 = Counter(sentence2)\n",
        "\n",
        "        all_words = set(vector1.keys()).union(set(vector2.keys()))\n",
        "\n",
        "        dot_product = sum(vector1.get(word, 0) * vector2.get(word, 0) for word in all_words)\n",
        "\n",
        "        magnitude1 = math.sqrt(sum(vector1.get(word, 0)**2 for word in all_words))\n",
        "        magnitude2 = math.sqrt(sum(vector2.get(word, 0)**2 for word in all_words))\n",
        "\n",
        "        if magnitude1 == 0 or magnitude2 == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return dot_product / (magnitude1 * magnitude2)\n",
        "\n",
        "    def jaccard_similarity(self, sentence1, sentence2):\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        words1 = [word.lower() for word in word_tokenize(sentence1) if word.isalnum() and word.lower() not in stop_words]\n",
        "        words2 = [word.lower() for word in word_tokenize(sentence2) if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "        intersection = len(set(words1).intersection(set(words2)))\n",
        "        union = len(set(words1).union(set(words2)))\n",
        "\n",
        "        if union == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return intersection / union\n",
        "\n",
        "    def word_movers_distance(self, sentence1, sentence2):\n",
        "        doc1 = self.nlp(sentence1)\n",
        "        doc2 = self.nlp(sentence2)\n",
        "\n",
        "        wmd = doc1.similarity(doc2)\n",
        "\n",
        "        return wmd\n",
        "\n",
        "    def sentence_similarity_transformers(self, sentence1, sentence2):\n",
        "        embeddings1 = self.model_paraphrase.encode(sentence1, convert_to_tensor=True)\n",
        "        embeddings2 = self.model_paraphrase.encode(sentence2, convert_to_tensor=True)\n",
        "\n",
        "        similarity_score = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "        return similarity_score.item()\n",
        "\n",
        "    def sentence_similarity_USE(self, sentence1, sentence2):\n",
        "        embeddings = self.use_model([sentence1, sentence2])\n",
        "\n",
        "        similarity_score = tf.keras.losses.cosine_similarity(embeddings[0], embeddings[1]).numpy()\n",
        "\n",
        "        return similarity_score\n",
        "\n",
        "    def create_doc2vec_model(self, sentences):\n",
        "        documents = [TaggedDocument(self.preprocess_text(sentence), [i]) for i, sentence in enumerate(sentences)]\n",
        "        model = Doc2Vec(documents, vector_size=100, window=5, min_count=1, workers=4)\n",
        "        return model\n",
        "\n",
        "    def sentence_similarity_doc2vec(self, sentence1, sentence2, model):\n",
        "        vec1 = model.infer_vector(self.preprocess_text(sentence1))\n",
        "        vec2 = model.infer_vector(self.preprocess_text(sentence2))\n",
        "\n",
        "        similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "        return similarity\n",
        "\n",
        "    def create_fasttext_model(self, sentences):\n",
        "        model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "        return model\n",
        "\n",
        "    def sentence_similarity_fasttext(self, sentence1, sentence2, model):\n",
        "        vec1 = model.wv[sentence1]\n",
        "        vec2 = model.wv[sentence2]\n",
        "\n",
        "        similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "        return similarity\n",
        "\n",
        "    def sentence_similarity_sbert(self, sentence1, sentence2):\n",
        "        embeddings1 = self.model_sbert.encode([sentence1])\n",
        "        embeddings2 = self.model_sbert.encode([sentence2])\n",
        "\n",
        "        similarity = np.dot(embeddings1[0], embeddings2[0]) / (np.linalg.norm(embeddings1[0]) * np.linalg.norm(embeddings2[0]))\n",
        "\n",
        "        return similarity\n",
        "\n",
        "    def sentence_similarity_distilbert(self, sentence1, sentence2):\n",
        "        embeddings1 = self.model_distilbert.encode([sentence1])\n",
        "        embeddings2 = self.model_distilbert.encode([sentence2])\n",
        "\n",
        "        similarity = np.dot(embeddings1[0], embeddings2[0]) / (np.linalg.norm(embeddings1[0]) * np.linalg.norm(embeddings2[0]))\n",
        "\n",
        "        return similarity\n",
        "\n",
        "    def sentence_similarity_laser(self, sentence1, sentence2):\n",
        "        embeddings1 = self.laser_model.embed_sentences([sentence1], lang='en')\n",
        "        embeddings2 = self.laser_model.embed_sentences([sentence2], lang='en')\n",
        "\n",
        "        similarity = np.dot(embeddings1[0], embeddings2[0]) / (np.linalg.norm(embeddings1[0]) * np.linalg.norm(embeddings2[0]))\n",
        "\n",
        "        return similarity\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def calculate_sentence_similarity(reference_sentence, student_answer):\n",
        "    def get_synonyms_antonyms(word):\n",
        "        synonyms = []\n",
        "        antonyms = []\n",
        "\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.append(lemma.name())\n",
        "                if lemma.antonyms():\n",
        "                    antonyms.append(lemma.antonyms()[0].name())\n",
        "\n",
        "        return synonyms, antonyms\n",
        "\n",
        "    def get_synonyms_antonyms_for_sentence(sentence):\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "\n",
        "        synonyms_list = []\n",
        "        antonyms_list = []\n",
        "\n",
        "        for word in words:\n",
        "            synonyms, antonyms = get_synonyms_antonyms(word)\n",
        "            synonyms_list.append(synonyms)\n",
        "            antonyms_list.append(antonyms)\n",
        "\n",
        "        result = {\n",
        "            \"sentence\": sentence,\n",
        "            \"words\": words,\n",
        "            \"synonyms\": dict(zip(words, synonyms_list)),\n",
        "            \"antonyms\": dict(zip(words, antonyms_list))\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    result1 = get_synonyms_antonyms_for_sentence(reference_sentence)\n",
        "    result2 = get_synonyms_antonyms_for_sentence(student_answer)\n",
        "\n",
        "    synonyms1 = result1[\"synonyms\"]\n",
        "    synonyms2 = result2[\"synonyms\"]\n",
        "\n",
        "    antonyms1 = result1[\"antonyms\"]\n",
        "    antonyms2 = result2[\"antonyms\"]\n",
        "\n",
        "    synonyms1_set = set([syn for synonyms in synonyms1.values() for syn in synonyms])\n",
        "    synonyms2_set = set([syn for synonyms in synonyms2.values() for syn in synonyms])\n",
        "\n",
        "    antonyms1_set = set([ant for antonyms in antonyms1.values() for ant in antonyms])\n",
        "    antonyms2_set = set([ant for antonyms in antonyms2.values() for ant in antonyms])\n",
        "\n",
        "    set1 = synonyms1_set.union(antonyms1_set)\n",
        "    set2 = synonyms2_set.union(antonyms2_set)\n",
        "\n",
        "    doc1 = nlp(\" \".join(set1))\n",
        "    doc2 = nlp(\" \".join(set2))\n",
        "\n",
        "    similarity_score = doc1.similarity(doc2)\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4Z2CwFaw4FR",
        "outputId": "d1b3265f-61a2-4cec-cb4a-280c429b952c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage\n",
        "similarity_models = SimilarityModels()\n",
        "\n",
        "sentence1 = \"happy to hear that\"\n",
        "sentence2 = \"sad to hear that\"\n",
        "\n",
        "sentences = [\n",
        "    \"happy to hear that\",\n",
        "    \"sad to hear that\"\n",
        "]\n",
        "\n",
        "reference_sentence = \"happy to hear that\"\n",
        "student_answer = \"sad to hear that\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "similarity1 = similarity_models.get_cosine_similarity(sentence1, sentence2)\n",
        "print(\"\\n Cosine Similarity:\", similarity1)\n",
        "\n",
        "similarity2 = similarity_models.jaccard_similarity(sentence1, sentence2)\n",
        "print(\"\\n Jaccard Similarity:\", similarity2)\n",
        "\n",
        "wmd_similarity = similarity_models.word_movers_distance(sentence1, sentence2)\n",
        "print(\"\\n Word Mover's Distance Similarity:\", wmd_similarity)\n",
        "\n",
        "similarity4 = similarity_models.sentence_similarity_transformers(sentence1, sentence2)\n",
        "print(\"\\n Sentence Transformers Similarity:\", similarity4)\n",
        "\n",
        "similarity5 = similarity_models.sentence_similarity_USE(sentence1, sentence2)\n",
        "print(\"\\n Universal Sentence Encoder Similarity:\", similarity5)\n",
        "\n",
        "\n",
        "model1 = similarity_models.create_doc2vec_model(sentences)\n",
        "similarity7 = similarity_models.sentence_similarity_doc2vec(sentence1, sentence2, model1)\n",
        "print(\"\\n Doc2Vec Cosine Similarity:\", similarity7)\n",
        "\n",
        "model2 = similarity_models.create_fasttext_model(sentences)\n",
        "similarity8 = similarity_models.sentence_similarity_fasttext(sentence1, sentence2, model2)\n",
        "print(\"\\n FastText Cosine Similarity:\", similarity8)\n",
        "\n",
        "similarity9 = similarity_models.sentence_similarity_sbert(sentence1, sentence2)\n",
        "print(\"\\n SBERT Similarity:\", similarity9)\n",
        "\n",
        "similarity10 = similarity_models.sentence_similarity_distilbert(sentence1, sentence2)\n",
        "print(\"\\n DistilBERT Similarity:\", similarity10)\n",
        "\n",
        "similarity11 = similarity_models.sentence_similarity_laser(sentence1, sentence2)\n",
        "print(\"\\n LASER Similarity:\", similarity11)\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "\n",
        "similarity_score = calculate_sentence_similarity(reference_sentence, student_answer)\n",
        "print(\"\\n Similarity Score:\", similarity_score)\n",
        "\n",
        "# Create a dictionary to store method names and their scores\n",
        "similarity_scores = {\n",
        "    \"Cosine Similarity\": similarity1,\n",
        "    \"Jaccard Similarity\": similarity2,\n",
        "    \"Word Mover's Distance Similarity\": wmd_similarity,\n",
        "    \"Sentence Transformers Similarity\": similarity4,\n",
        "    \"Universal Sentence Encoder Similarity\": similarity5,\n",
        "    \"Doc2Vec Cosine Similarity\": similarity7,\n",
        "    \"FastText Cosine Similarity\": similarity8,\n",
        "    \"SBERT Similarity\": similarity9,\n",
        "    \"DistilBERT Similarity\": similarity10,\n",
        "    \"LASER Similarity\": similarity11,\n",
        "    \"anonyms_score\":similarity_score\n",
        "}\n",
        "\n",
        "# Sort the dictionary by values (scores) in ascending order\n",
        "sorted_scores = {k: v for k, v in sorted(similarity_scores.items(), key=lambda item: item[1])}\n",
        "\n",
        "# Print the sorted scores\n",
        "print(\"\\n Sorted Similarity Scores:\")\n",
        "for method, score in sorted_scores.items():\n",
        "    print(f\"{method}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5MorWYdxpRS",
        "outputId": "9bd46d14-6160-4dd4-ee47-f19dacfcb7b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Cosine Similarity: 0.4999999999999999\n",
            "\n",
            " Jaccard Similarity: 0.3333333333333333\n",
            "\n",
            " Word Mover's Distance Similarity: 0.9300998605625794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-c03ef2409991>:72: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  wmd = doc1.similarity(doc2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sentence Transformers Similarity: 0.7601169347763062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Universal Sentence Encoder Similarity: -0.6496285\n",
            "\n",
            " Doc2Vec Cosine Similarity: 0.1918851\n",
            "\n",
            " FastText Cosine Similarity: 0.5376856\n",
            "\n",
            " SBERT Similarity: 0.42703322\n",
            "\n",
            " DistilBERT Similarity: 0.5009988\n",
            "\n",
            " LASER Similarity: 0.8913571\n",
            "\n",
            " Similarity Score: 0.880781536207257\n",
            "\n",
            " Sorted Similarity Scores:\n",
            "Universal Sentence Encoder Similarity: -0.6496285200119019\n",
            "Doc2Vec Cosine Similarity: 0.1918850988149643\n",
            "Jaccard Similarity: 0.3333333333333333\n",
            "SBERT Similarity: 0.4270332157611847\n",
            "Cosine Similarity: 0.4999999999999999\n",
            "DistilBERT Similarity: 0.5009987950325012\n",
            "FastText Cosine Similarity: 0.5376855731010437\n",
            "Sentence Transformers Similarity: 0.7601169347763062\n",
            "anonyms_score: 0.880781536207257\n",
            "LASER Similarity: 0.8913571238517761\n",
            "Word Mover's Distance Similarity: 0.9300998605625794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-c03ef2409991>:197: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity_score = doc1.similarity(doc2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average similarity score\n",
        "total_scores = len(sorted_scores)\n",
        "avg_similarity = sum(sorted_scores.values()) / total_scores\n",
        "print(\"\\n Average Similarity Score:\", avg_similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNWDpigbzoyf",
        "outputId": "213e916f-80a9-47f9-e35e-709e20179856"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Average Similarity Score: 0.48215117740264035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WBIEwzR02AJZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}